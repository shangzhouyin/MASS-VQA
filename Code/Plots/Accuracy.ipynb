{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "split = 'val'\n",
    "results = json.load(open('predictions.json'))\n",
    "dataset = json.load(open('../data_vizwiz/Annotations/%s.json'%split))\n",
    "\n",
    "img2gt = {x['image']:x['answers'] for x in dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset[:10])\n",
    "#results[:30]\n",
    "#dataset[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'answer': u'unsuitable', u'image': u'VizWiz_val_000000028000.jpg'}\n",
      "{u'answerable': 0, u'image': u'VizWiz_val_000000028000.jpg', u'question': u\"What's this?\", u'answers': [{u'answer_confidence': u'yes', u'answer': u'unsuitable'}, {u'answer_confidence': u'yes', u'answer': u'unsuitable'}, {u'answer_confidence': u'maybe', u'answer': u'beans'}, {u'answer_confidence': u'yes', u'answer': u'unanswerable'}, {u'answer_confidence': u'yes', u'answer': u'unsuitable'}, {u'answer_confidence': u'yes', u'answer': u'unanswerable'}, {u'answer_confidence': u'maybe', u'answer': u'unanswerable'}, {u'answer_confidence': u'yes', u'answer': u'unsuitable'}, {u'answer_confidence': u'yes', u'answer': u'unanswerable'}, {u'answer_confidence': u'maybe', u'answer': u'unsuitable'}], u'answer_type': u'unanswerable'}\n"
     ]
    }
   ],
   "source": [
    "print(results[0])\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz_val_000000028000.jpg\n"
     ]
    }
   ],
   "source": [
    "print(results[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz_val_000000028000.jpg\n",
      "VizWiz_val_000000028000.jpg\n"
     ]
    }
   ],
   "source": [
    "imdir='VizWiz_%s_%012d.jpg'\n",
    "print(dataset[0]['image'])\n",
    "img = imdir%(split,28000)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3173\n",
      "other : 0.594075007879\n",
      "number : 0.0151276394579\n",
      "unanswerable : 0.350772139931\n",
      "yes/no : 0.0400252127324\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "img2ans_type = {}\n",
    "for one_data in dataset:\n",
    "    ans_counter = Counter([x['answer'] for x in one_data['answers']])\n",
    "    ans = ans_counter.most_common(1)[0][0]\n",
    "    if ans == 'yes' or ans == 'no':\n",
    "        ans_type = 'yes/no'\n",
    "    elif ans == 'unanswerable' or ans == 'unsuitable':\n",
    "        ans_type = 'unanswerable'\n",
    "    elif ans.isdigit():\n",
    "        ans_type = 'number'\n",
    "    else:\n",
    "        ans_type = 'other'\n",
    "    img2ans_type[one_data['image']] = ans_type\n",
    "    \n",
    "all_ans = img2ans_type.values()\n",
    "print len(all_ans)\n",
    "for ans_type in set(all_ans):\n",
    "    print ans_type, ':', all_ans.count(ans_type)*1.0/len(all_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5123437335854606\n",
      "other : 0.3359858532272325\n",
      "number : 0.26388888888888884\n",
      "unanswerable : 0.7999401018268942\n",
      "yes/no : 0.7034120734908136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "img2acc = {}\n",
    "imdir='VizWiz_%s_%012d.jpg'\n",
    "\n",
    "for pred in results:\n",
    "    #img = imdir%(split,pred['question_id']) #Reconstruct image name from question_id\n",
    "    img = pred['image']\n",
    "    pred_ans = pred['answer']\n",
    "    gt_ans = img2gt[img]\n",
    "    gt_ans = [x['answer'] for x in gt_ans]\n",
    "    gt_ans = [x.lower() for x in gt_ans]\n",
    "    cur_acc = np.minimum(1.0, gt_ans.count(pred_ans)/3.0)\n",
    "    img2acc[img] = cur_acc\n",
    "\n",
    "print 'Accuracy :', np.mean(img2acc.values())\n",
    "for ans_type in set(all_ans):\n",
    "    acc_per_type = np.mean([acc for img, acc in img2acc.items() if img2ans_type[img] == ans_type])\n",
    "    print ans_type, ':', acc_per_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Denis prepro validation balanced'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Denis prepro validation balanced\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download coco-caption from https://github.com/tylin/coco-caption\n",
    "import sys\n",
    "sys.path.insert(0, '../coco-caption')\n",
    "\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "class COCOEvalCap:\n",
    "    def __init__(self,images,gts,res):\n",
    "        self.evalImgs = []\n",
    "        self.eval = {}\n",
    "        self.imgToEval = {}\n",
    "        self.params = {'image_id': images}\n",
    "        self.gts = gts\n",
    "        self.res = res\n",
    "\n",
    "    def evaluate(self):\n",
    "        imgIds = self.params['image_id']\n",
    "        gts = self.gts\n",
    "        res = self.res\n",
    "\n",
    "        # =================================================\n",
    "        # Set up scorers\n",
    "        # =================================================\n",
    "        print 'tokenization...'\n",
    "        tokenizer = PTBTokenizer()\n",
    "        gts  = tokenizer.tokenize(gts)\n",
    "        res = tokenizer.tokenize(res)\n",
    "\n",
    "        # =================================================\n",
    "        # Set up scorers\n",
    "        # =================================================\n",
    "        print 'setting up scorers...'\n",
    "        scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "            (Meteor(),\"METEOR\"),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            (Cider(), \"CIDEr\")\n",
    "        ]\n",
    "\n",
    "        # =================================================\n",
    "        # Compute scores\n",
    "        # =================================================\n",
    "        eval = {}\n",
    "        for scorer, method in scorers:\n",
    "            print 'computing %s score...'%(scorer.method())\n",
    "            assert(set(gts.keys()) == set(res.keys()))\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "            if type(method) == list:\n",
    "                for sc, scs, m in zip(score, scores, method):\n",
    "                    self.setEval(sc, m)\n",
    "                    self.setImgToEvalImgs(scs, imgIds, m)\n",
    "                    print \"%s: %0.3f\"%(m, sc)\n",
    "            else:\n",
    "                self.setEval(score, method)\n",
    "                self.setImgToEvalImgs(scores, imgIds, method)\n",
    "                print \"%s: %0.3f\"%(method, score)\n",
    "        self.setEvalImgs()\n",
    "\n",
    "    def setEval(self, score, method):\n",
    "        self.eval[method] = score\n",
    "\n",
    "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
    "        for imgId, score in zip(imgIds, scores):\n",
    "            if not imgId in self.imgToEval:\n",
    "                self.imgToEval[imgId] = {}\n",
    "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
    "            self.imgToEval[imgId][method] = score\n",
    "\n",
    "    def setEvalImgs(self):\n",
    "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 3627, 'guess': [3536, 364, 100, 32], 'testlen': 3536, 'correct': [2103, 166, 36, 11]}\n",
      "ratio: 0.974910394265\n",
      "Bleu_1: 0.580\n",
      "Bleu_2: 0.508\n",
      "Bleu_3: 0.449\n",
      "Bleu_4: 0.417\n",
      "computing METEOR score...\n",
      "METEOR: 0.307\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.593\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.707\n",
      "{'CIDEr': 0.7068554617442656, 'Bleu_4': 0.4171507568309097, 'Bleu_3': 0.44878194619936634, 'Bleu_2': 0.5075632436570343, 'Bleu_1': 0.5796292858195882, 'ROUGE_L': 0.5929457568277345, 'METEOR': 0.3074730242342619}\n"
     ]
    }
   ],
   "source": [
    "#res = {imdir%(split,x['question_id'])):[{'image_id':imdir%(split,x['question_id']), 'caption':x['answer']}] for x in results}\n",
    "res = {unicode(imdir%(split,x['question_id']), \"utf-8\"):[{'image_id':imdir%(split,x['question_id']), 'caption':x['answer']}] for x in results}\n",
    "\n",
    "gts = {}\n",
    "for img, ans_list in img2gt.items():\n",
    "    ans_list = [x['answer'] for x in ans_list]\n",
    "    tmp = []\n",
    "    for x in ans_list:\n",
    "        try:\n",
    "            tmp.append(str(x))\n",
    "        except:\n",
    "            pass\n",
    "    ans_list = tmp\n",
    "    ans_list = [{'image_id': img, 'caption': str(x)} for x in ans_list]\n",
    "    gts[img] = ans_list\n",
    "\n",
    "for img in gts.keys():\n",
    "    if img not in res.keys():\n",
    "        res[img] = [{'image_id':img, 'caption':''}]\n",
    "\n",
    "#### CHANGED CODE OF BLEU/METEOR SINCE RAISES ERROR WHEN COMPARING gts.keys() == res.keys()\n",
    "       \n",
    "evalObj = COCOEvalCap(gts.keys(),gts,res)\n",
    "evalObj.evaluate()\n",
    "print evalObj.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pkl\n",
    "prob = pkl.load(open('saved_model/%s_prob.pkl'%split))\n",
    "answer2answer_id = json.load(open('data/create_vocab/answer2answer_id.json'))\n",
    "unanswerable_labels = [answer2answer_id['unanswerable'], answer2answer_id['unsuitable']]\n",
    "img2answerable = {x['image']:x['answerable'] for x in dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP_rel: 0.8944\n",
      "AP_irrel: 0.5905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "y_test = []\n",
    "pred = []\n",
    "\n",
    "for res in results:\n",
    "    img = res['image']\n",
    "    gt_ans = img2answerable[img]\n",
    "    y_test.append(gt_ans)\n",
    "    one_prob = prob[img]\n",
    "    one_pred = 1 - sum([one_prob[x] for x in unanswerable_labels])\n",
    "    pred.append(one_pred)\n",
    "y_test = np.array(y_test)\n",
    "pred = np.array(pred)\n",
    "\n",
    "gt_labels = np.asarray(y_test) > 0.5\n",
    "precision, recall, thresholds = precision_recall_curve(gt_labels, pred)\n",
    "average_precision = average_precision_score(gt_labels, pred)\n",
    "print \"AP_rel: %.4f\"%average_precision\n",
    "with open('saved_model/results_rel.txt','w') as fid:\n",
    "    fid.write(str(average_precision))\n",
    "    fid.write('\\n')\n",
    "    fid.write('\\n'.join(['%.4f\\t%.4f\\t%.4f'%x for x in list(zip(recall,precision,thresholds))[::-1]]))\n",
    "\n",
    "\n",
    "gt_labels_n = np.asarray(y_test) < 0.5\n",
    "pred_n = 1.0 - pred\n",
    "precision, recall, thresholds = precision_recall_curve(gt_labels_n, pred_n)\n",
    "average_precision = average_precision_score(gt_labels_n, pred_n)\n",
    "print \"AP_irrel: %.4f\"%average_precision\n",
    "with open('saved_model/results_irrel.txt','w') as fid:\n",
    "    fid.write(str(average_precision))\n",
    "    fid.write('\\n')\n",
    "    fid.write('\\n'.join(['%.4f\\t%.4f\\t%.4f'%x for x in list(zip(recall,precision,thresholds))[::-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
